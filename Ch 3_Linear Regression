#Chapter 3: Linear Regression

#Libraries:
library(MASS) #very large collection of data sets and functions
library(ISLR) #data sets from book


############ Simple Linear Regression ##############

#The MASS library contains the Boston data set, which records medv (median house value) for 506 neighborhoods around Boston.
#We will seek to preict medv using 13 predictors such as rm (average number of rooms per house), age (average age of house), 
#and lstat (percent of households with low socioeconomic status).

fix(Boston)
names(Boston)

#We will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as
#the predictor. The basic syntax is lm(y~x, data), where y is the response, x is the predictor, and data is the
#data set in which the two variables are kept

attach(Boston) #attach to set as data set
lm.fit = lm(medv~lstat, data = Boston) #Or define data set in function call
summary(lm.fit)

names(lm.fit) #for other information
coef(lm.fit) #intercept and slope
confint(lm.fit) #confidence interval
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval = "confidence") #returns prediction intervals for medv for assigned values of lstat
predict(lm.fit, data.frame(lstat = c(5,10,15)), interval = "prediction")
#For example, the 95% confidence interval associated with lstat value of 10 is (24.47, 25.63) and the 95% prediction interval is (12.82, 37.27)

#plot the medv and lstat along with the least squares regression line
attach(Boston)
plot(lstat, medv)
abline(lm.fit)

#abline can be used to draw any line with intercept a and slope b with abline(a,b)
abline(lm.fit, lwd = 3) #increases line width by factor of 3
abline(lm.fit, lwd = 3, col = "red") #with color
plot(lstat, medv, col="red") #plot with red points
plot(lstat, medv, pch=20) #plot with filled dots
plot(lstat, medv, pch = "+") #plot with + for data points
plot(1:20, 1:20, pch = 1:20) #display of the top 20 data point symbols

#Four diagnostic plots are automatically produced by applying the plot() funtion directly to the output of lm():
#Residuals vs Fitted, Normal Q-Q, Scale-Location, Residuals vs Leverage

par(mfrow = c(2,2)) #divides the plotting region into a 2x2 grid of panels
plot(lm.fit)

#Alternatively we can compute the residuals from a linear regression line fit using the residuals() function.
#rstudent() will return the studentized residuals, and we can use this function to plot the resiguals against the fitted values

plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))

#On a basis of the residual plots, there is some vidence of non-linearity. Leverage statstics can be computed for any
#number of predictors using the hatvalues() functions

plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit)) #identifies the index of the largest element of a vector. In this case it tells us the observation has the largest leverage statistic


######### Multiple Linear Regression ############

#In order to fit a multiple linear regression model using least squares, we again use the lm() function.
#The syntax lm(y~x1+x2+x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function
#now outputs the regression coefficients for all the predictors.

lm.fit = lm(medv~lstat+age, data = Boston)
summary(lm.fit)

#The Boston data set contains 13 variables, and so it would be cumbersome to have to type all of these in order to perform a regression
#using all of the predictors. Instead we have the short hand:

lm.fit = lm(medv~., data = Boston)
summary(lm.fit)

#We can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence,
#summary(lm.fit)$r.sq gives us the R^2, and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be
#used to compute variance inflation factors. Most VIF's are low to moderate for this data. The car package is not part of the base R installation
#so it must be downloaded the first time you use it via the install.packages option in R

install.packages("car")
library(car)
vif(lm.fit)

#what if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age
#has a high p-value.

lm.fit1 = lm(medv~.-age, data = Boston)
summary(lm.fit1)

#alternatively, the update() function can be used

lm.fit1 = update(lm.fit, ~.-age)


############ Interaction Terms ###################

#It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells 
#R to include an interaction term between lstat and black. The syntax lstat*age simulataneously includes lstat, age,
#and the interaction term lstat x age as predictors; it is shorthand for lstat + age + lstat:age

summary(lm(medv~lstat*age, data=Boston))


########## Non-linear Transformations of the Predictors #############

#The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor
#X, we can create a predictor X^2 using I(X^2). The function I() is needed since the ^ has special meaning in a formula; 
#wrapping as we do allows the standard usage in R, which is to raise X to the powe r2. We now perform a regression
#of medv onto lstat and lstat^2.

lm.fit2 = lm(medv~lstat+I(lstat^2))
summary(lm.fit2)

#The near zero p-value associated with the quadratic term suggest it leads to an improved model. We use the anova() 
#function to further quantify the extent to which the quadratic fit is usperior.

lm.fit = lm(medv~lstat)
anova(lm.fit, lm.fit2)

#here model 1 represents the linear model and model 2 corresponds to the quadratic model. The anova() function performs a hypothesis
#test comparing the two models. The null hypothesis is that hte two models fit the data equally well, and the alternative is
#that the quadratic model is superior. Here the F-statistic is 135 and the associated p-value is virtually zero. This
#provides clear evidence that hte quadratic model is superior.

par(mfrow = c(2,2))
plot(lm.fit2)

#Now there is no discernible pattern in the residuals.

#In order to create a cubic fit, we can include a predictor of the form I(X^3) However, this approach can get cumbersome.
#A better approach involves using the poly() function to create the polynomial within lm(). For example, a 5th level polynomial:

lm.fit5 = lm(medv~poly(lstat,5))
summary(lm.fit5)

#This suggest that including teh additional polynomial terms, up to fifth order, leads to an improvement in the model fit.
#However, further investigation of this data would reveal no polynomial terms beyond fifth order have significant p-values.
#Of course we are not limited to polynomial transformations of predictors. Here we try a log transformation:

summary(lm(medv~log(rm), data=Boston))


################# Qualitative Predictors ################
#WE will now examine the Carseats data, which is part of the ISLR library. We will attempt to predict Sales (child car seats)
#in 400 locations based on a number of predictors

fix(Carseats)
names(Carseats)

#The Carseats data includes qualitative predictors usch as Shelveloc, an indicator of the quality of the shelving locations - 
#that is, the space within the store where the product is displayed at each location. The predictor Shelveloc takes 
#one of three values: Bad, Medium, Good.
#Given a qualitative variable, R generates dummy variables automatically.

lm.fit = lm(Sales~.+Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)

#The contrast() function returns the coding that R uses for the dummy variables

attach(Carseats)
contrasts(ShelveLoc)

#R has created a ShelveLoc dummy variable that atkes the value of 1 if the shelving is Good and 0 otherwise.
#The fact that the coefficient for the ShelveLocGood in the regression output is positive indicates that a good
#shelving location is associated with higher sales. ShelveLocMedium has a smaller coefficient indicating that a medium 
#shelving location leads to higher sales than a bad location but less than good.


################ Writing Functions #####################

#As we have seen, R comes with many useful functions, and still more functions are available by way of R libraries.
#However, we will often be interested in performing an operation for which no function is available. In this setting,
#we may want to write our own function. Below we provide a simple function that reads ISLR and MASS libraries, called
#LoadLibraries()

#Note the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are abou to 
#be input. Hittine ENTER after typing { will cause R to print the + symbol. We can then input as many commands as we wish
#hitting ENTER after each one and closing with }

LoadLibraries = function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}

}

